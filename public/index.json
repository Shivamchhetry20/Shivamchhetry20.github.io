[{"content":"Introduction This course covers topics related to profanity in Early Irish. It relates such profanity to profanity in modern English:\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fermentum turpis nec dolor dapibus, ac commodo enim consequat. Suspendisse potenti. Integer ac mauris nec ligula finibus ullamcorper. Phasellus rutrum enim in nunc aliquam mollis. Vivamus auctor nunc ac augue dapibus, in fermentum enim hendrerit. Duis et ultrices sem, eget efficitur lacus. Introductory video Lecture video 0 Introductory readings Paper 1 Paper 2 Part 1: Overview of Early Irish This section reviews the Early Irish language. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nLecture videos Lecture video 1 Lecture video 2 Lecture notes Lecture notes 1 – Concise overview of early Irish Lecture notes 2 – Complete and detailed overview of early Irish Readings Paper 3 Paper 4 Part 2: Profanity in Indo-European Languages This section introduces profanity in Indo-European languages. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua:\nMaecenas quis ex nec tortor pulvinar tristique. Morbi eget massa nec quam interdum posuere. Vestibulum lobortis auctor massa, sed hendrerit risus malesuada eu. Fusce nec metus ligula. Donec in mauris dolor. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris vestibulum erat nec odio tincidunt, vitae laoreet mi tempor.\nPellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris vestibulum erat nec odio tincidunt, vitae laoreet mi tempor. Quisque id eros sit amet enim tempor pellentesque. Suspendisse tincidunt risus a vehicula tincidunt. Nullam sollicitudin libero et ex cursus ultrices. Phasellus sit amet justo vitae nulla hendrerit pretium. Aenean quis velit eu risus pharetra vestibulum. Sed nec risus dolor. Vivamus vel leo ac ante pharetra auctor. Curabitur eleifend sapien non nisl varius, in hendrerit ipsum vehicula. Sed non justo sit amet nisi pharetra fermentum.\nLecture videos Lecture video 3 Lecture video 4 Compulsory readings Paper 5 Paper 6 Optional readings Paper 7 Paper 8 Paper 9 Homework Problem set on profanity in Indo-European languages Part 3: The originality of profanity in Early Irish This section turns to the various elements that make profanity in Early Irish so original. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit essecillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nLecture videos Lecture video 5 Lecture slides 6 Conclusion This section concludes. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"http://localhost:1313/courses/course2/","summary":"This undergraduate course discusses profanity in Early Irish, and relates such profanity to profanity in modern English.","title":""},{"content":" Mailing address Professor Dr von Igelfeld\nInstitute of Romance Philology\nUniversity of Regensburg\nRegensburg, Germany\nOffice address Room 133\nInstitute of Romance Philology\nUniversity of Regensburg\nOffice location ","permalink":"http://localhost:1313/location/","summary":"\u003chr\u003e\n\u003ch4 id=\"mailing-address\"\u003eMailing address\u003c/h4\u003e\n\u003cp\u003eProfessor Dr von Igelfeld\u003cbr\u003e\nInstitute of Romance Philology\u003cbr\u003e\nUniversity of Regensburg\u003cbr\u003e\nRegensburg, Germany\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"office-address\"\u003eOffice address\u003c/h4\u003e\n\u003cp\u003eRoom 133\u003cbr\u003e\nInstitute of Romance Philology\u003cbr\u003e\nUniversity of Regensburg\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"office-location\"\u003eOffice location\u003c/h4\u003e\n\u003ciframe src=\"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d10470.896334563153!2d12.085487114429176!3d48.99680799095555!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x479fc1126394f30f%3A0xb4c5000594ee5334!2sUniversity%20of%20Regensburg!5e0!3m2!1sen!2sus!4v1714871932562!5m2!1sen!2sus\" \r\nwidth=\"700\" height=\"500\" style=\"border:0;\" allowfullscreen=\"\" loading=\"lazy\"\u003e\u003c/iframe\u003e","title":"Location"},{"content":" Schedule Office hours take place on Wednesday at 10am.\nLocation By default meetings are in my office. I am also available for virtual meetings on Zoom.\nMeeting material If we are meeting to discuss research, please send me a written description of the work that you would like to discus. Presentation slides or paper draft are perfectly fine. If you do not have those, please send a one-page description of the research. Please send me the material by 8pm on the evening prior of our meeting. ","permalink":"http://localhost:1313/officehours/","summary":"\u003chr\u003e\n\u003ch4 id=\"schedule\"\u003eSchedule\u003c/h4\u003e\n\u003cp\u003eOffice hours take place on Wednesday at 10am.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"location\"\u003eLocation\u003c/h4\u003e\n\u003cp\u003eBy default meetings are in my office. I am also available for virtual meetings on Zoom.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"meeting-material\"\u003eMeeting material\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eIf we are meeting to discuss research, please send me a written description of the work that you would like to discus.\u003c/li\u003e\n\u003cli\u003ePresentation slides or paper draft are perfectly fine.\u003c/li\u003e\n\u003cli\u003eIf you do not have those, please send a one-page description of the research.\u003c/li\u003e\n\u003cli\u003ePlease send me the material by 8pm on the evening prior of our meeting.\u003c/li\u003e\n\u003c/ul\u003e","title":"Office Hours"},{"content":"Download Paper Raw data Unpacking the Architectural Revolution: A Deep Dive into the \u0026ldquo;Attention is All You Need\u0026rdquo; Paper\nFor over two decades, I’ve witnessed seismic shifts in AI—from perceptrons to CNNs, Boltzmann machines to LSTMs—but few innovations have reshaped the landscape as rapidly or profoundly as the transformer architecture introduced in the 2017 paper Attention is All You Need. What began as a novel approach to sequence transduction (think machine translation or text summarization) has since become the backbone of modern NLP, powering everything from chatbots to code generators.\nYet here’s what most retrospectives miss: the transformer wasn’t just an improvement—it was a paradigm shift. It discarded the recurrence and convolution-centric thinking that dominated sequence modeling, replacing them with a mechanism so elegantly simple that even its creators called it \u0026ldquo;surprisingly straightforward.\u0026rdquo; That mechanism? Scaled dot-product attention.\nIn this post, I’ll dissect why this paper remains a masterclass in minimalist design and how its core ideas solved four critical problems that plagued earlier architectures like RNNs and LSTMs:\nVanishing gradients in long sequences Computational inefficiency in sequential processing Context fragmentation (e.g., losing track of subject-verb agreement over paragraphs) Lack of parallelizability during training We’ll start by reconstructing the authors’ thought process: Why abandon recurrence entirely? How does self-attention create dynamic, context-aware word representations? And what makes transformers scale so ruthlessly efficiently with modern hardware?\nAlong the way, I’ll decode the paper’s technical jargon (query-key-value matrices, positional embeddings, multi-head attention) into intuitive first principles. No prior knowledge of attention mechanisms is assumed—just a familiarity with matrix operations and backpropagation.\nBy the end, you’ll not only understand the transformer’s machinery but also appreciate how its design choices foreshadowed the era of 100B-parameter models like GPT-4 and PaLM. Let’s begin.\n(Next, I’ll refine the technical deep dive based on your subsequent content. For now, this intro sets the stage by framing the transformer as a solution to well-known pain points while teasing insights that resonate with both researchers and practitioners.)\nFrom One-Hot Encoding to High-Dimensional Geometry: A Vectorized Perspective In this section, we’ll explore how one-hot encoding and matrix operations can be understood through the lens of high-dimensional geometry. By visualizing words as vectors in a multi-dimensional space, we can build intuition for how transformers and other machine learning models process language.\nOne-Hot Encoding: Words as Vectors Let’s start with the basics. One-hot encoding is a way to represent words as vectors in a high-dimensional space. If our vocabulary has ( V ) words, each word is represented as a vector in ( V )-dimensional space, where all elements are zero except for one element, which is set to 1.\nExample: A Tiny Vocabulary Consider a vocabulary of three words:\nfiles = ([1, 0, 0]) find = ([0, 1, 0]) my = ([0, 0, 1]) The sentence \u0026ldquo;Find my files\u0026rdquo; becomes:\nfind = ([0, 1, 0]) my = ([0, 0, 1]) files = ([1, 0, 0]) When stacked, this forms a matrix:\nEach word is a basis vector in this 3D space, pointing along one of the axes.\nDot Products: Measuring Similarity The dot product (or inner product) of two vectors is a measure of their similarity. For one-hot vectors:\nThe dot product of a vector with itself is 1 (maximum similarity). The dot product of two different one-hot vectors is 0 (no similarity). Geometric Interpretation In high-dimensional space, the dot product calculates the projection of one vector onto another. For one-hot vectors, this projection is either:\n1 if they point in the same direction (same word). 0 if they are orthogonal (different words). This property makes dot products useful for lookup operations and similarity comparisons.\nMatrix Multiplication: A Geometric Transformation Matrix multiplication is a way to transform vectors from one space to another. Let’s break it down geometrically.\nExample: Matrix as a Lookup Table Suppose we have a matrix ( B ) representing word embeddings: Here, each row of ( B ) is a 2D vector representing a word in a reduced space. Multiplying a one-hot vector by ( B ) extracts the corresponding row: This operation is like using a one-hot vector to look up a word’s embedding in a high-dimensional table.\nTransition Models: From Words to Sequences Now, let’s extend this to sequence modeling. A transition matrix describes the probability of moving from one word to another.\nFirst-Order Markov Model In a first-order Markov model, the probability of the next word depends only on the current word. This can be represented as a matrix where each row corresponds to a word, and each column represents the probability of transitioning to another word.\nExample: Transition Matrix For the vocabulary {show, me, my, files, directories, photos, please}, the transition matrix might look like this: Here, the row for my shows probabilities for transitioning to files (50%), directories (30%), or photos (20%).\nSecond-Order Markov Model: Adding Context A second-order Markov model considers the two most recent words to predict the next word. This increases the model’s context and reduces uncertainty.\nGeometric Interpretation In high-dimensional space, this corresponds to expanding the dimensionality of the transition matrix. Each row now represents a pair of words, and the columns represent the next word.\nExample: Second-Order Transition Matrix For the sentence \u0026ldquo;Check whether the battery ran down please,\u0026rdquo; the second-order model might look like this: This model eliminates ambiguity by considering more context, leading to sharper predictions.\nWhy This Matters for Transformers Transformers leverage these geometric principles to process sequences efficiently:\nOne-hot encoding provides a sparse, high-dimensional representation of words. Dot products measure similarity between words or sequences. Matrix multiplication enables efficient lookup and transformation of embeddings. Higher-order context (like in second-order models) is captured through mechanisms like self-attention, which dynamically weights the importance of different words in a sequence. By understanding these geometric foundations, we can better appreciate how transformers model language as a high-dimensional, context-aware system.\nIn the next section, we’ll dive into self-attention—the mechanism that allows transformers to dynamically focus on relevant parts of a sequence, enabling them to handle long-range dependencies and complex relationships.\nSecond-Order Sequence Models with Skips: Capturing Long-Range Dependencies In the previous section, we explored how second-order Markov models improve upon first-order models by considering pairs of words to predict the next word. However, language often involves long-range dependencies—relationships between words that span far beyond just the previous two. In this section, we’ll extend our understanding to second-order models with skips, a clever way to capture these dependencies without exploding the model’s complexity.\nThe Problem with Higher-Order Models Consider the following two sentences:\n\u0026ldquo;Check the program log and find out whether it ran please.\u0026rdquo; \u0026ldquo;Check the battery log and find out whether it ran down please.\u0026rdquo; Here, the word following ran depends on whether program or battery appeared earlier in the sentence—a dependency spanning 8 words. A naive approach would be to use an eighth-order Markov model, but this is computationally infeasible. For a vocabulary of size ( N ), an eighth-order model would require ( N^8 ) rows in its transition matrix—an astronomically large number.\nSecond-Order Models with Skips: A Sly Solution Instead of building a full eighth-order model, we can use a second-order model with skips. This approach considers pairs of words, but not necessarily adjacent ones. Specifically, it pairs the most recent word with each of the preceding words in the sequence.\nHow It Works Pair the most recent word with each preceding word: For example, in the sentence \u0026ldquo;Check the battery log and find out whether it ran,\u0026rdquo; we pair ran with battery, log, find, etc. Use these pairs as features: Each pair votes on the next word based on its learned weights. Sum the votes: The word with the highest total vote is predicted as the next word. Example: Transition Matrix with Skips Let’s focus on predicting the word after ran. The relevant pairs are:\nbattery, ran → predicts down with weight 1 and please with weight 0. program, ran → predicts please with weight 1 and down with weight 0. The transition matrix for these pairs might look like this: When we sum the votes:\nFor the sentence with battery, down gets 1 vote and please gets 0. For the sentence with program, please gets 1 vote and down gets 0. This approach correctly predicts the next word despite the long-range dependency.\nMasking: Sharpening Predictions While the above method works, it can produce weak predictions if many irrelevant features contribute small votes. To address this, we introduce masking—a technique to suppress uninformative features.\nHow Masking Works Identify relevant features: In our example, only battery, ran and program, ran are informative. Create a mask: A vector with 1s for relevant features and 0s for irrelevant ones. Apply the mask: Multiply the feature vector by the mask to zero out irrelevant votes. Example: Masked Transition Matrix After masking, the transition matrix becomes: [ \\begin{bmatrix} \\text{battery, ran} \u0026amp; \\rightarrow \u0026amp; \\text{down} = 1, \\text{please} = 0 \\ \\text{program, ran} \u0026amp; \\rightarrow \u0026amp; \\text{please} = 1, \\text{down} = 0 \\ \\end{bmatrix} ]\nNow, the predictions are sharp:\nbattery, ran → down with 100% confidence. program, ran → please with 100% confidence. This masking process is the essence of attention in transformers. It allows the model to focus on the most relevant parts of the input sequence.\nAttention as Matrix Multiplication To implement attention efficiently, transformers express everything as matrix multiplications. Here’s how:\nQuery (Q): Represents the feature of interest (e.g., the most recent word). Key (K): Represents the collection of masks (one for each word in the sequence). Dot Product (QK^T): Computes the similarity between the query and each key, effectively performing a differentiable lookup. Example: Mask Lookup Suppose we have a matrix ( K ) of mask vectors: [ K = \\begin{bmatrix} \\text{mask}_1 \\ \\text{mask}_2 \\ \\vdots \\ \\text{mask}_N \\ \\end{bmatrix} ]\nTo find the mask for the most recent word, we multiply its one-hot vector by ( K ): [ \\text{mask} = \\text{one-hot} \\cdot K ]\nThis operation is at the heart of the attention mechanism in transformers.\nWhy This Matters The second-order model with skips and masking provide an intuitive framework for understanding how transformers handle long-range dependencies. By focusing on relevant features and suppressing noise, transformers can efficiently process sequences and make accurate predictions.\nIn the next section, we’ll bridge the gap between this intuitive explanation and the actual implementation of transformers, focusing on how self-attention and multi-head attention enable these models to scale to massive datasets and complex tasks.\nKey Takeaways\nSecond-order models with skips capture long-range dependencies without exploding complexity. Masking sharpens predictions by focusing on relevant features. Attention is implemented as a differentiable lookup using matrix multiplications. Would you like to proceed with the next section, where we dive into self-attention and the full transformer architecture?\n","permalink":"http://localhost:1313/papers/paper4/","summary":"\u003ch5 id=\"download\"\u003eDownload\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\"\u003ePaper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/pmichaillat/recession-indicator\" target=\"_blank\"\u003eRaw data\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eUnpacking the Architectural Revolution: A Deep Dive into the \u0026ldquo;Attention is All You Need\u0026rdquo; Paper\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"attention_architecture.png\" alt=\"\"  /\u003e\r\n\u003c/p\u003e\n\u003cp\u003eFor over two decades, I’ve witnessed seismic shifts in AI—from perceptrons to CNNs, Boltzmann machines to LSTMs—but few innovations have reshaped the landscape as rapidly or profoundly as the transformer architecture introduced in the 2017 paper \u003cem\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\"\u003eAttention is All You Need\u003c/a\u003e\u003c/em\u003e. What began as a novel approach to sequence transduction (think machine translation or text summarization) has since become the backbone of modern NLP, powering everything from chatbots to code generators.\u003c/p\u003e","title":"Attention is all you need"},{"content":" Download Paper Raw data Abstract Using several case studies, this paper describes the inner hedgehog, a psychological condition widespread in academic occupations. The condition has lasting consequences and no known cure. Mauris tincidunt quam a libero consequat, nec pharetra nunc tristique. Pellentesque eget ipsum ut dui laoreet congue ut nec nulla. Nulla facilisi. Sed consequat, odio ac aliquet tempor, turpis augue auctor mauris, at malesuada sem dolor eget libero. Nullam iaculis malesuada risus, id fringilla quam sagittis ac. Fusce congue vel ex et facilisis. Integer volutpat eros ut urna efficitur, id efficitur sapien pharetra.\nCitation Schreiber-Ziegler, Hilda, and Moritz-Maria von Igelfeld. 2021. \u0026ldquo;Your Inner Hedgehog.\u0026rdquo; Journal of Socio-Experimental Psychology 131 (2): 1299–1302.\n@article{SZI21, author = {Hilda Schreiber-Ziegler and Moritz-Maria von Igelfeld}, year = {2021}, title ={Your Inner Hedgehog}, journal = {Journal of Socio-Experimental Psychology}, volume = {131}, number = {2}, pages = {1299--1302}} Related material Nontechnical summary ","permalink":"http://localhost:1313/papers/paper3/","summary":"Using several case studies, this paper describes the inner hedgehog, a psychological condition widespread in academic occupations. The condition has lasting consequences and no known cure.","title":"Your Inner Hedgehog"},{"content":"1. Introduction Problem Statement:\n\u0026ldquo;WhatsApp group chats contain a wealth of data about user behavior, but extracting meaningful insights from raw text exports is tedious. Manual analysis of hundreds/thousands of messages is impractical.\u0026rdquo; Your Solution:\n\u0026ldquo;I built a Python web app that automates WhatsApp chat analysis and visualizes key metrics like active users, message patterns, and timelines. It’s deployed on Heroku for easy access.\u0026rdquo; 2. Key Features to Highlight Chat Parsing\nHandles WhatsApp’s specific text export format (timestamps, users, multi-line messages). Regex-based extraction of metadata (e.g., \\d+/\\d+/\\d+, \\d+:\\d+ - (.*?): (.*)). Metrics Calculated\nMost active users (message count, media shares). Daily/hourly activity patterns. Timeline of messages (e.g., peaks during weekends). Visualizations\nInteractive charts (Bar graphs, timelines). Word clouds for frequent terms (if implemented). Deployment\nHeroku integration for public access. Flask backend for lightweight web handling. 3. Technical Deep Dive (Code Breakdown) Include snippets with explanations:\n# app.py – Core Flask Logic @app.route(\u0026#39;/analyze\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def analyze(): file = request.files[\u0026#39;file\u0026#39;] chat_text = file.read().decode(\u0026#39;utf-8\u0026#39;) # Parse uploaded file messages = parse_whatsapp_chat(chat_text) # Custom parsing function df = pd.DataFrame(messages) # Pandas for analysis generate_plots(df) # Matplotlib/Plotly visualization return render_template(\u0026#39;results.html\u0026#39;, data=df) # Parsing Logic Example (Adapt from your code) def parse_whatsapp_chat(text): pattern = r\u0026#39;(\\d+/\\d+/\\d+, \\d+:\\d+) - (.*?): (.*)\u0026#39; messages = re.findall(pattern, text) return [{\u0026#39;timestamp\u0026#39;: msg[0], \u0026#39;user\u0026#39;: msg[1], \u0026#39;content\u0026#39;: msg[2]} for msg in messages] 4. Deployment Challenges \u0026amp; Solutions Heroku Setup:\n\u0026ldquo;I used a Procfile to define the web process and requirements.txt to manage dependencies. Heroku’s ephemeral filesystem required temporary storage for uploaded files.\u0026rdquo;\nScalability:\n\u0026ldquo;The app uses lightweight libraries (Pandas, Matplotlib) to handle large chat exports efficiently.\u0026rdquo;\n5. Screenshots/Visuals Include 2-3 visuals:\nUpload interface (upload.html). Sample dashboard with charts (results.html). Heroku deployment workflow diagram. 6. Future Improvements Advanced Analytics:\n\u0026ldquo;Add sentiment analysis using NLTK or spaCy to gauge chat tone.\u0026rdquo; User Authentication:\n\u0026ldquo;Implement login via OAuth to save user-specific chat histories.\u0026rdquo; Real-Time Features:\n\u0026ldquo;WebSocket integration for live chat analysis during active group conversations.\u0026rdquo; 7. Call-to-Action Demo Link: https://your-heroku-app.herokuapp.com GitHub Repo: github.com/shivamkc01/WhatsAppChat_AnalysisDeployusingHeroku\n\u0026ldquo;Feel free to contribute or fork the repository!\u0026rdquo; Blog Title Suggestions \u0026ldquo;Building a WhatsApp Chat Analyzer: From Raw Exports to Actionable Insights\u0026rdquo; \u0026ldquo;Deploying a Python WhatsApp Analytics Tool on Heroku: A Step-by-Step Guide\u0026rdquo; \u0026ldquo;Visualizing Group Chat Dynamics with Python and Flask\u0026rdquo; Additional Tips Personal Anecdote: Share why you built this (e.g., \u0026ldquo;I wanted to analyze my college group’s chat activity to identify key contributors\u0026rdquo;). Metrics: Include performance stats (e.g., \u0026ldquo;Processes 10k messages in \u0026lt;5 seconds\u0026rdquo;). SEO Keywords: \u0026ldquo;WhatsApp analysis\u0026rdquo;, \u0026ldquo;Python Flask\u0026rdquo;, \u0026ldquo;Heroku deployment\u0026rdquo;, \u0026ldquo;Chat visualization\u0026rdquo;. Let me know if you’d like me to draft specific sections or refine the technical explanations! 🚀\n","permalink":"http://localhost:1313/courses/course1/","summary":"This graduate course presents classical results in Romance philology. it focuses especially on Portugese and Spanish irregular verbs.","title":"WhatsApp Chat Analysis with Deployment in herokuapp"},{"content":"Overview This dataset contains all irregular verbs in all known Romance languages—including Portugese, Spanish, French, and Italian. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nView dataset Irregular verbs in Portugese: data Irregular verbs in Italian: data Irregular verbs in French: data Irregular verbs in Spanish: data Source of data Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nUsing data with Python Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nStart Python: Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nimport numpy as np import pandas as pd Open the file: Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat data.csv.\nfile_path = \u0026#39;data.csv\u0026#39; with open(file_path, \u0026#39;r\u0026#39;) as file: Read data: Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nlines = file.readlines() Parse and process data: Duis aute line_data irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur data.extend.\ndata = [] for line in lines: line_data = line.strip().split(\u0026#39;,\u0026#39;) # Split the line into a list of values line_data = [float(value) for value in line_data] # Convert values to floats data.extend(line_data) # Extend the main list with values from the line Compute summary statistics using NumPy: Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum: data_array.\ndata_array = np.array(data) # Convert the list to a NumPy array mean = np.mean(data_array) median = np.median(data_array) std_dev = np.std(data_array) min_value = np.min(data_array) max_value = np.max(data_array) Display summary statistics: Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat print.\nprint(f\u0026#34;Mean: {mean}\u0026#34;) print(f\u0026#34;Median: {median}\u0026#34;) print(f\u0026#34;Standard Deviation: {std_dev}\u0026#34;) print(f\u0026#34;Minimum Value: {min_value}\u0026#34;) print(f\u0026#34;Maximum Value: {max_value}\u0026#34;) Description of simulation parameters Parameter Value Language Time period Description $\\alpha$ $1/2$ French 1930–1954 Tempor dolor in $\\lambda$ $e/2$ French 1930–1954 Fugiat sint occaecat $\\gamma$ $\\ln(3)$ Spanish 1833–1954 Duis officia deserunt $\\omega$ $10^{-4}$ Italian 1930–1994 Excepteur et dolore magna aliqua $\\sigma$ $1.5$ Portuguese 1990–2023 Lorem culpa qui $\\chi^2$ $\\pi^2$ Portuguese 1990–2023 Labore et dolore ","permalink":"http://localhost:1313/data/data1/","summary":"This dataset contains all irregular verbs in known Romance languages.","title":"List of Irregular Verbs Across Romance Languages"},{"content":" Download Paper Online appendix Code and data Abstract This paper reviews unusual uses for olive oil throughout the Mediterranean world. It highlights in particular the challengs arising from excessive or unorthodox consumption of olive oil. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nFigure 6: Some Uses For Olive Oil Citation Unterholzer, Detlev A., and Moritz-Maria von Igelfeld. 2013. \u0026ldquo;Unusual Uses For Olive Oil.\u0026rdquo; Journal of Oleic Science 34 (1): 449–489. http://www.alexandermccallsmith.com/book/unusual-uses-for-olive-oil.\n@article{UI13, author = {Detlev A. Unterholzer and Moritz-Maria von Igelfeld}, year = {2013}, title ={Unusual Uses For Olive Oil}, journal = {Journal of Oleic Science}, volume = {34}, number = {1}, pages = {449--489}, url = {http://www.alexandermccallsmith.com/book/unusual-uses-for-olive-oil}} Related material Presentation slides Summary of the paper ","permalink":"http://localhost:1313/papers/paper1/","summary":"This paper reviews unusual uses for olive oil throughout the Mediterranean world. It highlights in particular the challengs arising from excessive or unorthodox consumption of olive oil.","title":"Unusual Uses For Olive Oil"},{"content":" Download Paper Online appendix Code and data Abstract This paper studies the pulmonary efficiency of sausage dogs. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur: $\\sin(\\theta) = x^2 - \\exp(1+\\chi)$. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor $\\zeta$ incididunt ut labore et dolore magna aliqua: $p(x) = \\int \\cos(\\zeta) d\\zeta - \\theta$. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nFigure 2: Dimensions of a sausage dog Citation Prinzel, Florianus, and Moritz-Maria von Igelfeld. 2004. \u0026ldquo;The Finer Points of Sausage Dogs.\u0026rdquo; Journal of Canine Science 43 (2): 89–109. http://www.alexandermccallsmith.com/book/the-finer-points-of-sausage-dogs.\n@article{PI04, author = {Florianus Prinzel and Moritz-Maria von Igelfeld}, year = {2004}, title ={The Finer Points of Sausage Dogs}, journal = {Journal of Canine Science}, volume = {43}, number = {2}, pages = {89--109}, url = {http://www.alexandermccallsmith.com/book/the-finer-points-of-sausage-dogs}} Related material Presentation slides Wikipedia entry ","permalink":"http://localhost:1313/papers/paper2/","summary":"This paper studies the pulmonary efficiency of sausage dogs through several experiments.","title":"The Finer Points of Sausage Dogs"},{"content":" Description This book discusses Portugese irregular verbs in great details. It is the seminal work on Romance philology.1 The book is the result of years of research into the etymology and vagaries of Portuguese verbs.2 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nPraise There is nothing more to be said on this subject. Nothing – Anonymous reviewer\nView Chapter 1: History of the Portuguese language Chapter 2: Review of regular verbs Chapter 3: Analysis of irregular verbs Excerpt from Chapter 1: Basic mathematical results from Portuguese philology Lorem ipsum dolor sit amet, consectetur adipiscing elit $x\\in \\mathbb{N}$. Proin ac libero nec eros accumsan sagittis: $x^\\ast = \\max f(x)$. Fusce gravida $4 \\ln(x+y) =4 \\int \\ln(x^2)dx$, lectus nec aliquet malesuada, augue dui lacinia velit, eget ullamcorper lorem lorem id turpis. Nam interdum est id venenatis fermentum: . Nullam fermentum, arcu eu luctus fermentum, felis orci pretium mi, eu bibendum ligula neque in metus.\n$$1+\\lambda\\exp{\\frac{\\beta}{\\alpha^2}} = \\max_{t\\in\\mathbb{R}}(x(t)-y(t)+z(t)^2).$$\nNulla facilisi. Phasellus pharetra ligula sit amet diam viverra, sed scelerisque ligula cursus. Curabitur sit amet libero eu velit fringilla vulputate, $2\\ln(x)$. Suspendisse potenti. Quisque imperdiet arcu ac nibh gravida, id posuere ligula efficitur. Curabitur posuere, dui at finibus viverra, felis justo pulvinar urna, id finibus sem purus eget orci.\nVestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Mauris vel neque at lorem fermentum tincidunt: $\u0026lt;x,y\u0026gt; = 2x^2 + \\mathcal{A}$. Etiam volutpat, risus at aliquet varius, sapien quam vulputate lectus, id ultrices lorem arcu ut magna. Sed malesuada scelerisque dignissim. Aliquam erat volutpat. Proin efficitur tincidunt nulla, a convallis magna cursus sit amet. Donec eget convallis libero. Pellentesque tincidunt nunc et nisi lacinia, quis auctor lorem suscipit:\n$$2\\exp(\\gamma) = 2\\exp(\\zeta/3) = 2\\exp(\\kappa).$$\nPraesent at eros a sapien sagittis scelerisque at a nulla. Aliquam erat volutpat. In hac habitasse platea dictumst. Phasellus congue vestibulum nisl, vitae accumsan est. Suspendisse potenti. In ut nunc ac quam congue cursus. Duis fermentum hendrerit eros, ut auctor velit pharetra ut. Nulla facilisi. Cras at convallis purus, a convallis mauris. Donec id est vel ipsum hendrerit laoreet.\nCitation Shivam Chhetry. 1997. Portugese Irregular Verbs. Regensburg, Germany: Regensburg University Press. http://www.alexandermccallsmith.com/book/portuguese-irregular-verbs.\n@book{I97, author = {Shivam Chhetry}, year = {1997}, title = {Portugese Irregular Verbs}, publisher = {Regensburg University Press}, address = {Regensburg, Germany}, url = {http://www.alexandermccallsmith.com/book/portuguese-irregular-verbs}} The acknowledged aim of the book is to dwarf all other books in the field.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAs a result of such intensive research, the book\u0026rsquo;s length is almost twelve hundred pages.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/books/book1/","summary":"This book discusses Portugese irregular verbs in great details.","title":"Portugese Irregular Verbs"}]