<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Attention is all you need | Shivam Chhetry</title>
<meta name="keywords" content="Deep Learning, tranfomers">
<meta name="description" content="This paper describes the inner hedgehog, a psychological condition widespread in academia. Published in the Journal of Socio-Experimental Psychology, 2021.">
<meta name="author" content="Shivam Chhetry">
<link rel="canonical" href="http://localhost:1313/papers/paper4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6cf4a8fa527330d9574f36d8d000fdaf90ca838ff09ab72fc27d3cb7ca1ddc5.css" integrity="sha256-1s9Kj6UnMw2VdPNtjQAP2vkMqDj/Caty/CfTy3yh3cU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/papers/paper4/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Attention is all you need" />
<meta property="og:description" content="This paper describes the inner hedgehog, a psychological condition widespread in academia. Published in the Journal of Socio-Experimental Psychology, 2021." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/papers/paper4/" />
<meta property="og:image" content="http://localhost:1313/papers/paper4/transformer.png" /><meta property="article:section" content="papers" />
<meta property="article:published_time" content="2021-04-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/papers/paper4/transformer.png" />
<meta name="twitter:title" content="Attention is all you need"/>
<meta name="twitter:description" content="This paper describes the inner hedgehog, a psychological condition widespread in academia. Published in the Journal of Socio-Experimental Psychology, 2021."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Papers",
      "item": "http://localhost:1313/papers/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Attention is all you need",
      "item": "http://localhost:1313/papers/paper4/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Attention is all you need",
  "name": "Attention is all you need",
  "description": "This paper describes the inner hedgehog, a psychological condition widespread in academia. Published in the Journal of Socio-Experimental Psychology, 2021.",
  "keywords": [
    "Deep Learning", "tranfomers"
  ],
  "articleBody": "Download Paper Raw data Unpacking the Architectural Revolution: A Deep Dive into the “Attention is All You Need” Paper\nFor over two decades, I’ve witnessed seismic shifts in AI—from perceptrons to CNNs, Boltzmann machines to LSTMs—but few innovations have reshaped the landscape as rapidly or profoundly as the transformer architecture introduced in the 2017 paper Attention is All You Need. What began as a novel approach to sequence transduction (think machine translation or text summarization) has since become the backbone of modern NLP, powering everything from chatbots to code generators.\nYet here’s what most retrospectives miss: the transformer wasn’t just an improvement—it was a paradigm shift. It discarded the recurrence and convolution-centric thinking that dominated sequence modeling, replacing them with a mechanism so elegantly simple that even its creators called it “surprisingly straightforward.” That mechanism? Scaled dot-product attention.\nIn this post, I’ll dissect why this paper remains a masterclass in minimalist design and how its core ideas solved four critical problems that plagued earlier architectures like RNNs and LSTMs:\nVanishing gradients in long sequences Computational inefficiency in sequential processing Context fragmentation (e.g., losing track of subject-verb agreement over paragraphs) Lack of parallelizability during training We’ll start by reconstructing the authors’ thought process: Why abandon recurrence entirely? How does self-attention create dynamic, context-aware word representations? And what makes transformers scale so ruthlessly efficiently with modern hardware?\nAlong the way, I’ll decode the paper’s technical jargon (query-key-value matrices, positional embeddings, multi-head attention) into intuitive first principles. No prior knowledge of attention mechanisms is assumed—just a familiarity with matrix operations and backpropagation.\nBy the end, you’ll not only understand the transformer’s machinery but also appreciate how its design choices foreshadowed the era of 100B-parameter models like GPT-4 and PaLM. Let’s begin.\n(Next, I’ll refine the technical deep dive based on your subsequent content. For now, this intro sets the stage by framing the transformer as a solution to well-known pain points while teasing insights that resonate with both researchers and practitioners.)\nFrom One-Hot Encoding to High-Dimensional Geometry: A Vectorized Perspective In this section, we’ll explore how one-hot encoding and matrix operations can be understood through the lens of high-dimensional geometry. By visualizing words as vectors in a multi-dimensional space, we can build intuition for how transformers and other machine learning models process language.\nOne-Hot Encoding: Words as Vectors Let’s start with the basics. One-hot encoding is a way to represent words as vectors in a high-dimensional space. If our vocabulary has ( V ) words, each word is represented as a vector in ( V )-dimensional space, where all elements are zero except for one element, which is set to 1.\nExample: A Tiny Vocabulary Consider a vocabulary of three words:\nfiles = ([1, 0, 0]) find = ([0, 1, 0]) my = ([0, 0, 1]) The sentence “Find my files” becomes:\nfind = ([0, 1, 0]) my = ([0, 0, 1]) files = ([1, 0, 0]) When stacked, this forms a matrix:\nEach word is a basis vector in this 3D space, pointing along one of the axes.\nDot Products: Measuring Similarity The dot product (or inner product) of two vectors is a measure of their similarity. For one-hot vectors:\nThe dot product of a vector with itself is 1 (maximum similarity). The dot product of two different one-hot vectors is 0 (no similarity). Geometric Interpretation In high-dimensional space, the dot product calculates the projection of one vector onto another. For one-hot vectors, this projection is either:\n1 if they point in the same direction (same word). 0 if they are orthogonal (different words). This property makes dot products useful for lookup operations and similarity comparisons.\nMatrix Multiplication: A Geometric Transformation Matrix multiplication is a way to transform vectors from one space to another. Let’s break it down geometrically.\nExample: Matrix as a Lookup Table Suppose we have a matrix ( B ) representing word embeddings: Here, each row of ( B ) is a 2D vector representing a word in a reduced space. Multiplying a one-hot vector by ( B ) extracts the corresponding row: This operation is like using a one-hot vector to look up a word’s embedding in a high-dimensional table.\nTransition Models: From Words to Sequences Now, let’s extend this to sequence modeling. A transition matrix describes the probability of moving from one word to another.\nFirst-Order Markov Model In a first-order Markov model, the probability of the next word depends only on the current word. This can be represented as a matrix where each row corresponds to a word, and each column represents the probability of transitioning to another word.\nExample: Transition Matrix For the vocabulary {show, me, my, files, directories, photos, please}, the transition matrix might look like this: Here, the row for my shows probabilities for transitioning to files (50%), directories (30%), or photos (20%).\nSecond-Order Markov Model: Adding Context A second-order Markov model considers the two most recent words to predict the next word. This increases the model’s context and reduces uncertainty.\nGeometric Interpretation In high-dimensional space, this corresponds to expanding the dimensionality of the transition matrix. Each row now represents a pair of words, and the columns represent the next word.\nExample: Second-Order Transition Matrix For the sentence “Check whether the battery ran down please,” the second-order model might look like this: This model eliminates ambiguity by considering more context, leading to sharper predictions.\nWhy This Matters for Transformers Transformers leverage these geometric principles to process sequences efficiently:\nOne-hot encoding provides a sparse, high-dimensional representation of words. Dot products measure similarity between words or sequences. Matrix multiplication enables efficient lookup and transformation of embeddings. Higher-order context (like in second-order models) is captured through mechanisms like self-attention, which dynamically weights the importance of different words in a sequence. By understanding these geometric foundations, we can better appreciate how transformers model language as a high-dimensional, context-aware system.\nIn the next section, we’ll dive into self-attention—the mechanism that allows transformers to dynamically focus on relevant parts of a sequence, enabling them to handle long-range dependencies and complex relationships.\nSecond-Order Sequence Models with Skips: Capturing Long-Range Dependencies In the previous section, we explored how second-order Markov models improve upon first-order models by considering pairs of words to predict the next word. However, language often involves long-range dependencies—relationships between words that span far beyond just the previous two. In this section, we’ll extend our understanding to second-order models with skips, a clever way to capture these dependencies without exploding the model’s complexity.\nThe Problem with Higher-Order Models Consider the following two sentences:\n“Check the program log and find out whether it ran please.” “Check the battery log and find out whether it ran down please.” Here, the word following ran depends on whether program or battery appeared earlier in the sentence—a dependency spanning 8 words. A naive approach would be to use an eighth-order Markov model, but this is computationally infeasible. For a vocabulary of size ( N ), an eighth-order model would require ( N^8 ) rows in its transition matrix—an astronomically large number.\nSecond-Order Models with Skips: A Sly Solution Instead of building a full eighth-order model, we can use a second-order model with skips. This approach considers pairs of words, but not necessarily adjacent ones. Specifically, it pairs the most recent word with each of the preceding words in the sequence.\nHow It Works Pair the most recent word with each preceding word: For example, in the sentence “Check the battery log and find out whether it ran,” we pair ran with battery, log, find, etc. Use these pairs as features: Each pair votes on the next word based on its learned weights. Sum the votes: The word with the highest total vote is predicted as the next word. Example: Transition Matrix with Skips Let’s focus on predicting the word after ran. The relevant pairs are:\nbattery, ran → predicts down with weight 1 and please with weight 0. program, ran → predicts please with weight 1 and down with weight 0. The transition matrix for these pairs might look like this: When we sum the votes:\nFor the sentence with battery, down gets 1 vote and please gets 0. For the sentence with program, please gets 1 vote and down gets 0. This approach correctly predicts the next word despite the long-range dependency.\nMasking: Sharpening Predictions While the above method works, it can produce weak predictions if many irrelevant features contribute small votes. To address this, we introduce masking—a technique to suppress uninformative features.\nHow Masking Works Identify relevant features: In our example, only battery, ran and program, ran are informative. Create a mask: A vector with 1s for relevant features and 0s for irrelevant ones. Apply the mask: Multiply the feature vector by the mask to zero out irrelevant votes. Example: Masked Transition Matrix After masking, the transition matrix becomes: [ \\begin{bmatrix} \\text{battery, ran} \u0026 \\rightarrow \u0026 \\text{down} = 1, \\text{please} = 0 \\ \\text{program, ran} \u0026 \\rightarrow \u0026 \\text{please} = 1, \\text{down} = 0 \\ \\end{bmatrix} ]\nNow, the predictions are sharp:\nbattery, ran → down with 100% confidence. program, ran → please with 100% confidence. This masking process is the essence of attention in transformers. It allows the model to focus on the most relevant parts of the input sequence.\nAttention as Matrix Multiplication To implement attention efficiently, transformers express everything as matrix multiplications. Here’s how:\nQuery (Q): Represents the feature of interest (e.g., the most recent word). Key (K): Represents the collection of masks (one for each word in the sequence). Dot Product (QK^T): Computes the similarity between the query and each key, effectively performing a differentiable lookup. Example: Mask Lookup Suppose we have a matrix ( K ) of mask vectors: [ K = \\begin{bmatrix} \\text{mask}_1 \\ \\text{mask}_2 \\ \\vdots \\ \\text{mask}_N \\ \\end{bmatrix} ]\nTo find the mask for the most recent word, we multiply its one-hot vector by ( K ): [ \\text{mask} = \\text{one-hot} \\cdot K ]\nThis operation is at the heart of the attention mechanism in transformers.\nWhy This Matters The second-order model with skips and masking provide an intuitive framework for understanding how transformers handle long-range dependencies. By focusing on relevant features and suppressing noise, transformers can efficiently process sequences and make accurate predictions.\nIn the next section, we’ll bridge the gap between this intuitive explanation and the actual implementation of transformers, focusing on how self-attention and multi-head attention enable these models to scale to massive datasets and complex tasks.\nKey Takeaways\nSecond-order models with skips capture long-range dependencies without exploding complexity. Masking sharpens predictions by focusing on relevant features. Attention is implemented as a differentiable lookup using matrix multiplications. Would you like to proceed with the next section, where we dive into self-attention and the full transformer architecture?\n",
  "wordCount" : "1786",
  "inLanguage": "en",
  "image":"http://localhost:1313/papers/paper4/transformer.png","datePublished": "2021-04-06T00:00:00Z",
  "dateModified": "2024-10-18T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Shivam Chhetry"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/papers/paper4/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shivam Chhetry",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Shivam Chhetry">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Shivam Chhetry</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/courses/" title="Courses">
                    <span>Courses</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/data/" title="Data">
                    <span>Data</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/books/" title="Books">
                    <span>Books</span>
                </a>
            </li>
            <li>
                <a href="https://en.wikipedia.org/wiki/Moritz-Maria_von_Igelfeld_%28character%29" title="Blog">
                    <span>Blog</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Attention is all you need
    </h1>
    <div class="post-meta"><span title='2021-04-06 00:00:00 +0000 UTC'>April 2021</span>&nbsp;&middot;&nbsp;Shivam Chhetry

</div>
  </header> 
  <div class="post-content"><h5 id="download">Download</h5>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" target="_blank">Paper</a></li>
<li><a href="https://github.com/pmichaillat/recession-indicator" target="_blank">Raw data</a></li>
</ul>
<hr>
<p><strong>Unpacking the Architectural Revolution: A Deep Dive into the &ldquo;Attention is All You Need&rdquo; Paper</strong></p>
<p><img loading="lazy" src="attention_architecture.png" alt=""  />
</p>
<p>For over two decades, I’ve witnessed seismic shifts in AI—from perceptrons to CNNs, Boltzmann machines to LSTMs—but few innovations have reshaped the landscape as rapidly or profoundly as the transformer architecture introduced in the 2017 paper <em><a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention is All You Need</a></em>. What began as a novel approach to sequence transduction (think machine translation or text summarization) has since become the backbone of modern NLP, powering everything from chatbots to code generators.</p>
<p>Yet here’s what most retrospectives miss: <strong>the transformer wasn’t just an improvement—it was a paradigm shift</strong>. It discarded the recurrence and convolution-centric thinking that dominated sequence modeling, replacing them with a mechanism so elegantly simple that even its creators called it &ldquo;surprisingly straightforward.&rdquo; That mechanism? <strong>Scaled dot-product attention</strong>.</p>
<p>In this post, I’ll dissect why this paper remains a masterclass in minimalist design and how its core ideas solved four critical problems that plagued earlier architectures like RNNs and LSTMs:</p>
<ol>
<li><strong>Vanishing gradients</strong> in long sequences</li>
<li><strong>Computational inefficiency</strong> in sequential processing</li>
<li><strong>Context fragmentation</strong> (e.g., losing track of subject-verb agreement over paragraphs)</li>
<li><strong>Lack of parallelizability</strong> during training</li>
</ol>
<p>We’ll start by reconstructing the authors’ thought process: Why abandon recurrence entirely? How does self-attention create dynamic, context-aware word representations? And what makes transformers <em>scale</em> so ruthlessly efficiently with modern hardware?</p>
<p>Along the way, I’ll decode the paper’s technical jargon (query-key-value matrices, positional embeddings, multi-head attention) into intuitive first principles. No prior knowledge of attention mechanisms is assumed—just a familiarity with matrix operations and backpropagation.</p>
<p>By the end, you’ll not only understand the transformer’s machinery but also appreciate how its design choices foreshadowed the era of 100B-parameter models like GPT-4 and PaLM. Let’s begin.</p>
<p><em>(Next, I’ll refine the technical deep dive based on your subsequent content. For now, this intro sets the stage by framing the transformer as a solution to well-known pain points while teasing insights that resonate with both researchers and practitioners.)</em></p>
<h1 id="from-one-hot-encoding-to-high-dimensional-geometry-a-vectorized-perspective">From One-Hot Encoding to High-Dimensional Geometry: A Vectorized Perspective</h1>
<p>In this section, we’ll explore how <strong>one-hot encoding</strong> and <strong>matrix operations</strong> can be understood through the lens of <strong>high-dimensional geometry</strong>. By visualizing words as vectors in a multi-dimensional space, we can build intuition for how transformers and other machine learning models process language.</p>
<hr>
<h2 id="one-hot-encoding-words-as-vectors">One-Hot Encoding: Words as Vectors</h2>
<p>Let’s start with the basics. <strong>One-hot encoding</strong> is a way to represent words as vectors in a high-dimensional space. If our vocabulary has ( V ) words, each word is represented as a vector in ( V )-dimensional space, where all elements are zero except for one element, which is set to 1.</p>
<h3 id="example-a-tiny-vocabulary">Example: A Tiny Vocabulary</h3>
<p>Consider a vocabulary of three words:</p>
<ul>
<li><strong>files</strong> = ([1, 0, 0])</li>
<li><strong>find</strong> = ([0, 1, 0])</li>
<li><strong>my</strong> = ([0, 0, 1])</li>
</ul>
<p>The sentence &ldquo;Find my files&rdquo; becomes:</p>
<ul>
<li><strong>find</strong> = ([0, 1, 0])</li>
<li><strong>my</strong> = ([0, 0, 1])</li>
<li><strong>files</strong> = ([1, 0, 0])</li>
</ul>
<p>When stacked, this forms a matrix:</p>
<p><img loading="lazy" src="fig1.png" alt=""  />
</p>
<p>Each word is a <strong>basis vector</strong> in this 3D space, pointing along one of the axes.</p>
<hr>
<h2 id="dot-products-measuring-similarity">Dot Products: Measuring Similarity</h2>
<p>The <strong>dot product</strong> (or inner product) of two vectors is a measure of their similarity. For one-hot vectors:</p>
<ul>
<li>The dot product of a vector with itself is <strong>1</strong> (maximum similarity).</li>
<li>The dot product of two different one-hot vectors is <strong>0</strong> (no similarity).</li>
</ul>
<h3 id="geometric-interpretation">Geometric Interpretation</h3>
<p>In high-dimensional space, the dot product calculates the <strong>projection</strong> of one vector onto another. For one-hot vectors, this projection is either:</p>
<ul>
<li><strong>1</strong> if they point in the same direction (same word).</li>
<li><strong>0</strong> if they are orthogonal (different words).</li>
</ul>
<p>This property makes dot products useful for <strong>lookup operations</strong> and <strong>similarity comparisons</strong>.</p>
<hr>
<h2 id="matrix-multiplication-a-geometric-transformation">Matrix Multiplication: A Geometric Transformation</h2>
<p>Matrix multiplication is a way to transform vectors from one space to another. Let’s break it down geometrically.</p>
<h3 id="example-matrix-as-a-lookup-table">Example: Matrix as a Lookup Table</h3>
<p>Suppose we have a matrix ( B ) representing word embeddings:
<img loading="lazy" src="fig2.png" alt=""  />
</p>
<p>Here, each row of ( B ) is a 2D vector representing a word in a reduced space. Multiplying a one-hot vector by ( B ) extracts the corresponding row:
<img loading="lazy" src="fig3.png" alt=""  />

This operation is like using a one-hot vector to <strong>look up</strong> a word’s embedding in a high-dimensional table.</p>
<hr>
<h2 id="transition-models-from-words-to-sequences">Transition Models: From Words to Sequences</h2>
<p>Now, let’s extend this to <strong>sequence modeling</strong>. A <strong>transition matrix</strong> describes the probability of moving from one word to another.</p>
<h3 id="first-order-markov-model">First-Order Markov Model</h3>
<p>In a first-order Markov model, the probability of the next word depends only on the current word. This can be represented as a matrix where each row corresponds to a word, and each column represents the probability of transitioning to another word.</p>
<h4 id="example-transition-matrix">Example: Transition Matrix</h4>
<p>For the vocabulary {show, me, my, files, directories, photos, please}, the transition matrix might look like this:
<img loading="lazy" src="fig4.png" alt=""  />
</p>
<p>Here, the row for <strong>my</strong> shows probabilities for transitioning to <strong>files</strong> (50%), <strong>directories</strong> (30%), or <strong>photos</strong> (20%).</p>
<hr>
<h2 id="second-order-markov-model-adding-context">Second-Order Markov Model: Adding Context</h2>
<p>A <strong>second-order Markov model</strong> considers the <strong>two most recent words</strong> to predict the next word. This increases the model’s context and reduces uncertainty.</p>
<h3 id="geometric-interpretation-1">Geometric Interpretation</h3>
<p>In high-dimensional space, this corresponds to expanding the dimensionality of the transition matrix. Each row now represents a <strong>pair of words</strong>, and the columns represent the next word.</p>
<h4 id="example-second-order-transition-matrix">Example: Second-Order Transition Matrix</h4>
<p>For the sentence &ldquo;Check whether the battery ran down please,&rdquo; the second-order model might look like this:
<img loading="lazy" src="fig5.png" alt=""  />
</p>
<p>This model eliminates ambiguity by considering more context, leading to sharper predictions.</p>
<hr>
<h2 id="why-this-matters-for-transformers">Why This Matters for Transformers</h2>
<p>Transformers leverage these geometric principles to process sequences efficiently:</p>
<ol>
<li><strong>One-hot encoding</strong> provides a sparse, high-dimensional representation of words.</li>
<li><strong>Dot products</strong> measure similarity between words or sequences.</li>
<li><strong>Matrix multiplication</strong> enables efficient lookup and transformation of embeddings.</li>
<li><strong>Higher-order context</strong> (like in second-order models) is captured through mechanisms like <strong>self-attention</strong>, which dynamically weights the importance of different words in a sequence.</li>
</ol>
<p>By understanding these geometric foundations, we can better appreciate how transformers model language as a high-dimensional, context-aware system.</p>
<hr>
<p>In the next section, we’ll dive into <strong>self-attention</strong>—the mechanism that allows transformers to dynamically focus on relevant parts of a sequence, enabling them to handle long-range dependencies and complex relationships.</p>
<h1 id="second-order-sequence-models-with-skips-capturing-long-range-dependencies">Second-Order Sequence Models with Skips: Capturing Long-Range Dependencies</h1>
<p>In the previous section, we explored how <strong>second-order Markov models</strong> improve upon first-order models by considering pairs of words to predict the next word. However, language often involves <strong>long-range dependencies</strong>—relationships between words that span far beyond just the previous two. In this section, we’ll extend our understanding to <strong>second-order models with skips</strong>, a clever way to capture these dependencies without exploding the model’s complexity.</p>
<hr>
<h2 id="the-problem-with-higher-order-models">The Problem with Higher-Order Models</h2>
<p>Consider the following two sentences:</p>
<ol>
<li>&ldquo;Check the program log and find out whether it ran please.&rdquo;</li>
<li>&ldquo;Check the battery log and find out whether it ran down please.&rdquo;</li>
</ol>
<p>Here, the word following <strong>ran</strong> depends on whether <strong>program</strong> or <strong>battery</strong> appeared earlier in the sentence—a dependency spanning <strong>8 words</strong>. A naive approach would be to use an <strong>eighth-order Markov model</strong>, but this is computationally infeasible. For a vocabulary of size ( N ), an eighth-order model would require ( N^8 ) rows in its transition matrix—an astronomically large number.</p>
<hr>
<h2 id="second-order-models-with-skips-a-sly-solution">Second-Order Models with Skips: A Sly Solution</h2>
<p>Instead of building a full eighth-order model, we can use a <strong>second-order model with skips</strong>. This approach considers pairs of words, but not necessarily adjacent ones. Specifically, it pairs the <strong>most recent word</strong> with <strong>each of the preceding words</strong> in the sequence.</p>
<h3 id="how-it-works">How It Works</h3>
<ol>
<li><strong>Pair the most recent word with each preceding word</strong>: For example, in the sentence &ldquo;Check the battery log and find out whether it ran,&rdquo; we pair <strong>ran</strong> with <strong>battery</strong>, <strong>log</strong>, <strong>find</strong>, etc.</li>
<li><strong>Use these pairs as features</strong>: Each pair votes on the next word based on its learned weights.</li>
<li><strong>Sum the votes</strong>: The word with the highest total vote is predicted as the next word.</li>
</ol>
<h3 id="example-transition-matrix-with-skips">Example: Transition Matrix with Skips</h3>
<p>Let’s focus on predicting the word after <strong>ran</strong>. The relevant pairs are:</p>
<ul>
<li><strong>battery, ran</strong> → predicts <strong>down</strong> with weight 1 and <strong>please</strong> with weight 0.</li>
<li><strong>program, ran</strong> → predicts <strong>please</strong> with weight 1 and <strong>down</strong> with weight 0.</li>
</ul>
<p>The transition matrix for these pairs might look like this:
<img loading="lazy" src="fig6.png" alt=""  />
</p>
<p>When we sum the votes:</p>
<ul>
<li>For the sentence with <strong>battery</strong>, <strong>down</strong> gets 1 vote and <strong>please</strong> gets 0.</li>
<li>For the sentence with <strong>program</strong>, <strong>please</strong> gets 1 vote and <strong>down</strong> gets 0.</li>
</ul>
<p>This approach correctly predicts the next word despite the long-range dependency.</p>
<hr>
<h2 id="masking-sharpening-predictions">Masking: Sharpening Predictions</h2>
<p>While the above method works, it can produce weak predictions if many irrelevant features contribute small votes. To address this, we introduce <strong>masking</strong>—a technique to suppress uninformative features.</p>
<h3 id="how-masking-works">How Masking Works</h3>
<ol>
<li><strong>Identify relevant features</strong>: In our example, only <strong>battery, ran</strong> and <strong>program, ran</strong> are informative.</li>
<li><strong>Create a mask</strong>: A vector with 1s for relevant features and 0s for irrelevant ones.</li>
<li><strong>Apply the mask</strong>: Multiply the feature vector by the mask to zero out irrelevant votes.</li>
</ol>
<h3 id="example-masked-transition-matrix">Example: Masked Transition Matrix</h3>
<p>After masking, the transition matrix becomes:
[
\begin{bmatrix}
\text{battery, ran} &amp; \rightarrow &amp; \text{down} = 1, \text{please} = 0 \
\text{program, ran} &amp; \rightarrow &amp; \text{please} = 1, \text{down} = 0 \
\end{bmatrix}
]</p>
<p>Now, the predictions are sharp:</p>
<ul>
<li><strong>battery, ran</strong> → <strong>down</strong> with 100% confidence.</li>
<li><strong>program, ran</strong> → <strong>please</strong> with 100% confidence.</li>
</ul>
<p>This masking process is the essence of <strong>attention</strong> in transformers. It allows the model to focus on the most relevant parts of the input sequence.</p>
<hr>
<h2 id="attention-as-matrix-multiplication">Attention as Matrix Multiplication</h2>
<p>To implement attention efficiently, transformers express everything as <strong>matrix multiplications</strong>. Here’s how:</p>
<ol>
<li><strong>Query (Q)</strong>: Represents the feature of interest (e.g., the most recent word).</li>
<li><strong>Key (K)</strong>: Represents the collection of masks (one for each word in the sequence).</li>
<li><strong>Dot Product (QK^T)</strong>: Computes the similarity between the query and each key, effectively performing a differentiable lookup.</li>
</ol>
<h3 id="example-mask-lookup">Example: Mask Lookup</h3>
<p>Suppose we have a matrix ( K ) of mask vectors:
[
K = \begin{bmatrix}
\text{mask}_1 \
\text{mask}_2 \
\vdots \
\text{mask}_N \
\end{bmatrix}
]</p>
<p>To find the mask for the most recent word, we multiply its one-hot vector by ( K ):
[
\text{mask} = \text{one-hot} \cdot K
]</p>
<p>This operation is at the heart of the <strong>attention mechanism</strong> in transformers.</p>
<hr>
<h2 id="why-this-matters">Why This Matters</h2>
<p>The <strong>second-order model with skips</strong> and <strong>masking</strong> provide an intuitive framework for understanding how transformers handle long-range dependencies. By focusing on relevant features and suppressing noise, transformers can efficiently process sequences and make accurate predictions.</p>
<p>In the next section, we’ll bridge the gap between this intuitive explanation and the actual implementation of transformers, focusing on how <strong>self-attention</strong> and <strong>multi-head attention</strong> enable these models to scale to massive datasets and complex tasks.</p>
<hr>
<p><strong>Key Takeaways</strong></p>
<ul>
<li><strong>Second-order models with skips</strong> capture long-range dependencies without exploding complexity.</li>
<li><strong>Masking</strong> sharpens predictions by focusing on relevant features.</li>
<li><strong>Attention</strong> is implemented as a differentiable lookup using matrix multiplications.</li>
</ul>
<p>Would you like to proceed with the next section, where we dive into <strong>self-attention</strong> and the full transformer architecture?</p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="http://localhost:1313/tags/tranfomers/">Tranfomers</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on x"
            href="https://x.com/intent/tweet/?text=Attention%20is%20all%20you%20need&amp;url=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f&amp;hashtags=DeepLearning%2ctranfomers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f&amp;title=Attention%20is%20all%20you%20need&amp;summary=Attention%20is%20all%20you%20need&amp;source=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f&title=Attention%20is%20all%20you%20need">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on whatsapp"
            href="https://api.whatsapp.com/send?text=Attention%20is%20all%20you%20need%20-%20http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on telegram"
            href="https://telegram.me/share/url?text=Attention%20is%20all%20you%20need&amp;url=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Attention%20is%20all%20you%20need&u=http%3a%2f%2flocalhost%3a1313%2fpapers%2fpaper4%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Shivam Chhetry</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
